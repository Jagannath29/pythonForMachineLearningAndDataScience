{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Accuracy\n",
    "# def accuracy(actual, predicted):\n",
    "#     correct = sum(1 for act, pred in zip(actual, predicted) if act == pred)\n",
    "#     total = len(actual)\n",
    "\n",
    "#     return correct / total\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Error\n",
    "\n",
    "# def error(actual, predicted): \n",
    "#     return 1 - accuracy(actual, predicted)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recall\n",
    "# def recall(actual, predicted):\n",
    "#     true_positive = sum(1 for act, pred in zip(actual, predicted) if act == pred == 1)\n",
    "#     actual_positive = sum(actual)\n",
    "#     return true_positive / actual_positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.86\n",
      "Error: -32.86\n",
      "Recall: [25.9787234  41.6        30.36842105]\n",
      "Precision: [23.48076923 52.         25.08695652]\n",
      "F1-score: [24.66666667 46.22222222 27.47619048]\n",
      "Weighted Average: 33.00084656084657\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    # Convert input lists to numpy arrays\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Initialize variables for each metric\n",
    "    tp = np.diag(np.dot(actual, predicted.T))\n",
    "    fp = np.sum(predicted, axis=0) - tp\n",
    "    fn = np.sum(actual, axis=0) - tp\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    # Calculate F1-score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.sum(np.diag(np.dot(actual, predicted.T))) / np.sum(np.sum(actual))\n",
    "\n",
    "    # Calculate error\n",
    "    error = 1 - accuracy\n",
    "\n",
    "    # Calculate weighted average\n",
    "    weights = np.sum(actual, axis=1) / np.sum(np.sum(actual))\n",
    "    weighted_average = np.dot(f1_score, weights)\n",
    "\n",
    "    return accuracy, error, recall, precision, f1_score, weighted_average\n",
    "\n",
    "# Actual values\n",
    "actual = [[34, 13, 5], [0, 52, 0], [13, 0, 33]]\n",
    "\n",
    "# Predicted values\n",
    "predicted = [[34, 0, 13], [13, 52, 0], [5, 0, 33]]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy, error, recall, precision, f1_score, weighted_average = calculate_metrics(actual, predicted)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Error:\", error)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1-score:\", f1_score)\n",
    "print(\"Weighted Average:\", weighted_average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
